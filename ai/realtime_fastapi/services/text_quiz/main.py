import os
import re
import ast
import json
import time
import pandas as pd
import logging
import openai
from quiz_generator import QuizGenerator
from dotenv import load_dotenv, find_dotenv
from hcx_tuning import CreateTaskExecutor


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(find_dotenv())

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# prompt.json ë¡œë“œ
with open(
    f"{os.getenv('FastAPI_DIR')}/services/text_quiz/prompt.json", "r", encoding="utf-8"
) as f:
    prompt_data = json.load(f)

# ìš”ì²­ ì œí•œ ì„¤ì •
REQUEST_LIMIT = 8  # 1ë¶„ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜
SLEEP_TIME = 30  # 30ì´ˆ ëŒ€ê¸°
BATCH_SIZE = 50  # 50ê°œì”© ì €ì¥ í›„ ë‹¤ìŒ ì‘ì—… ì§„í–‰
MAX_RETRIES = 5  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
RETRY_DELAY = 10  # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
# MAX_RETRIES, REQUEST_LIMIT, SLEEP_TIME, RETRY_DELAY, BATCH_SIZE, logger, prompt_data, QuizGenerator ë“±ì€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •


def extract_json_from_markdown(text):
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë‚´ JSON ë¬¸ìì—´ ì¶”ì¶œ
    pattern = r"```(?:json)?\s*([\s\S]+?)\s*```"
    match = re.search(pattern, text)
    if match:
        return match.group(1)
    return text


def parse_json_response(response_text):
    # ì²«ë²ˆì§¸ ì‹œë„: ê¸°ë³¸ json.loads
    try:
        return json.loads(response_text)
    except json.JSONDecodeError:
        pass
    # ë‘ë²ˆì§¸ ì‹œë„: ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ë¡œ ë³€ê²½ í›„ ì‹œë„
    try:
        corrected = response_text.replace("'", '"')
        return json.loads(corrected)
    except json.JSONDecodeError:
        pass
    # ì„¸ë²ˆì§¸ ì‹œë„: ast.literal_eval ì‚¬ìš© (JSON í˜•ì‹ê³¼ ìœ ì‚¬í•œ íŒŒì´ì¬ ë¦¬í„°ëŸ´)
    try:
        return ast.literal_eval(response_text)
    except Exception:
        raise ValueError("JSON ë³€í™˜ ì‹¤íŒ¨")


def generate_tuning_dataset():
    api_keys = {
        "openai": os.getenv("OPENAI_API_KEY"),
        "google": os.getenv("GOOGLE_API_KEY"),
        "naver": os.getenv("NAVER_API_KEY"),
    }
    print(api_keys)

    csv_path = f"{os.getenv('FastAPI_DIR')}/services/text_quiz/data/text.csv"
    context_data = pd.read_csv(csv_path)

    quiz_gen = QuizGenerator(
        api_keys, "gemini-2.0-flash-thinking-exp-01-21", context_data
    )

    tuning_output_file = (
        f"{os.getenv('FastAPI_DIR')}/services/text_quiz/data/tuning_quiz_dataset.jsonl"
    )
    db_output_file = (
        f"{os.getenv('FastAPI_DIR')}/services/text_quiz/data/db_quiz_dataset.jsonl"
    )

    # "a" ëª¨ë“œë¡œ íŒŒì¼ì„ ì—´ë©´ ê¸°ì¡´ì— ì €ì¥ëœ ë‚´ìš© ìœ„ì— ë®ì–´ì“°ì§€ ì•Šê³  ì´ì–´ì„œ ì €ì¥í•¨
    with open(tuning_output_file, "a", encoding="utf-8") as tuning_f, open(
        db_output_file, "a", encoding="utf-8"
    ) as db_f:

        request_count = 0
        c_id = 0

        for idx in range(len(context_data)):
            text_id = int(context_data.iloc[idx]["text_id"])
            title = context_data.iloc[idx]["title"]
            category = context_data.iloc[idx]["category"]
            content = context_data.iloc[idx]["content"]

            for level in range(1, 6):
                prompt_with_level = f"{prompt_data['quiz_prompt']}\n\në‚œì´ë„: {level}\n\nì œëª©: {title}\nì¹´í…Œê³ ë¦¬: {category}\në³¸ë¬¸: {content}\n\n"
                retries = 0
                while retries < MAX_RETRIES:
                    try:
                        _, generated_quiz = quiz_gen.generate_quiz(
                            prompt_with_level, idx
                        )
                        raw_quiz = generated_quiz.strip()
                        json_str = extract_json_from_markdown(raw_quiz)
                        quiz_json = parse_json_response(json_str)
                        if not all(
                            k in quiz_json
                            for k in [
                                "level",
                                "question",
                                "answer",
                                "answer_explain",
                                "options",
                            ]
                        ):
                            logger.error("ğŸš¨ ì‘ë‹µ JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ")
                            raise ValueError("ì‘ë‹µ JSON í˜•ì‹ ì˜¤ë¥˜")
                        tuning_data = {
                            "C_ID": c_id,
                            "T_ID": 0,
                            "Text": prompt_with_level,
                            "Completion": raw_quiz,
                        }
                        tuning_f.write(
                            json.dumps(tuning_data, ensure_ascii=False) + "\n"
                        )
                        db_data = {
                            "text_id": text_id,
                            "level": quiz_json["level"],
                            "question": quiz_json["question"],
                            "answer": quiz_json["answer"],
                            "answer_explain": quiz_json["answer_explain"],
                            "options": quiz_json["options"],
                        }
                        db_f.write(json.dumps(db_data, ensure_ascii=False) + "\n")
                        request_count += 1
                        logger.info(
                            f"âœ… ë°ì´í„° ìƒ˜í”Œ (text_id={text_id}, level={level}) ì €ì¥ ì™„ë£Œ"
                        )
                        time.sleep(8)
                        if request_count >= REQUEST_LIMIT:
                            logger.info(f"â³ {SLEEP_TIME}ì´ˆ ë™ì•ˆ ëŒ€ê¸° ì¤‘...")
                            time.sleep(SLEEP_TIME)
                            request_count = 0
                        break
                    except openai.RateLimitError as e:
                        if hasattr(e, "headers"):
                            retry_after = int(e.headers.get("Retry-After", RETRY_DELAY))
                        else:
                            logger.warning(
                                "RateLimitErrorì— headers ì†ì„±ì´ ì—†ìŒ. ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì‚¬ìš©"
                            )
                            retry_after = RETRY_DELAY
                        logger.warning(
                            f"âš ï¸ API ìš”ì²­ ì œí•œ ì´ˆê³¼! {retry_after}ì´ˆ í›„ ì¬ì‹œë„..."
                        )
                        time.sleep(retry_after)
                        retries += 1
                    except openai.OpenAIError as e:
                        logger.error(f"ğŸš¨ OpenAI API ì—ëŸ¬ ë°œìƒ: {e}")
                        time.sleep(RETRY_DELAY)
                        retries += 1
                    except ValueError as e:
                        logger.error(f"ğŸš¨ {str(e)}")
                        retries += 1
                if retries >= MAX_RETRIES:
                    logger.error(
                        f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼! text_id={text_id}, level={level} ìŠ¤í‚µ"
                    )
                c_id += 1
            if (idx + 1) % BATCH_SIZE == 0:
                logger.info("ğŸš€ 50ê°œ ì €ì¥ ì™„ë£Œ! ë‹¤ìŒ ì‘ì—… ì§„í–‰ ì¤‘...")
    logger.info(
        f"ğŸ¯ í€´ì¦ˆ ë°ì´í„°ì…‹ì´ {tuning_output_file} ë° {db_output_file} íŒŒì¼ì— JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
    )


def tune_model():
    task_executor = CreateTaskExecutor()

    # íŠœë‹ íŒŒë¼ë¯¸í„° ì„¤ì •
    task_name = "text_quiz_model_tuning"
    model = "HCX-003"  # ì‚¬ìš©í•  ëª¨ë¸ (ë³€ê²½ ê°€ëŠ¥)
    train_epochs = "8"
    learning_rate = "1e-4"
    dataset_file_path = "tuning_quiz_dataset.jsonl"  # JSONL ë°ì´í„°ì…‹ ê²½ë¡œ

    logger.info(
        f"ğŸ” íŠœë‹ ì‹œì‘: {task_name}, ëª¨ë¸: {model}, Epochs: {train_epochs}, LR: {learning_rate}"
    )

    # íŠœë‹ ìš”ì²­ ì‹¤í–‰
    tuning_result = task_executor.execute(
        task_name, model, train_epochs, learning_rate, dataset_file_path
    )

    # íŠœë‹ ê²°ê³¼ ì¶œë ¥
    logger.info(f"ğŸš€ íŠœë‹ ê²°ê³¼: {tuning_result}")

    print(tuning_result)


if __name__ == "__main__":
    try:
        # 1. í€´ì¦ˆ ë°ì´í„° ìƒì„±
        generate_tuning_dataset()
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ: {e}")

    # 2. ë°ì´í„° ìƒì„± ì™„ë£Œ í›„, íŠœë‹ ì‹¤í–‰
    tune_model()
