# 3. HCX-003 vs (Instruction Tuning)HCX-003
import json, os, sys, time
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

sys.path.append("C:/level4-nlp-finalproject-hackathon-nlp-04-lv3/ai")
from batch_airflow.tuning.llm_api import OpenAIApi


class VocabExplainEvaluator:
    def __init__(self):
        self._api_key = os.getenv("GEMINI_API_KEY")
        self._base_url = os.getenv("GEMINI_BASE_URL")

    def query_gemini(self, content):
        openai_api = OpenAIApi(self._api_key, base_url=self._base_url)

        prompt = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ëŠë¦° í•™ìŠµìë¥¼ ìœ„í•œ í•œêµ­ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
            },
            {"role": "user", "content": content},
        ]
        response = openai_api.call(
            prompt, model_name="gemini-2.0-flash-thinking-exp", structured_output=None
        )

        # responseê°€ JSONì¸ì§€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•œ í›„ ë³€í™˜
        if isinstance(response, str):
            parsed_response = self.parse_response(response)
        else:
            parsed_response = response  # ì´ë¯¸ JSONì¸ ê²½ìš°

        return parsed_response

    def parse_response(self, response_text):
        try:
            if isinstance(response_text, dict):
                return response_text  # ì´ë¯¸ JSONì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

            response_text = response_text.strip()

            # ```json íƒœê·¸ ì œê±°
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]

            return json.loads(response_text)  # ë³€í™˜ ì‹œë„
        except json.JSONDecodeError:
            print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨: ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return None

    def evaluate_response(self, original_data, tuned_data):
        """Gemini Flash Thinkerë¥¼ ì‚¬ìš©í•´ HCX ì›ë³¸ ëª¨ë¸ vs íŠœë‹ ëª¨ë¸ ë¹„êµ í‰ê°€"""
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‹¨ì–´ ì„¤ëª…ì´ ëŠë¦°í•™ìŠµìì—ê²Œ ì í•©í•œì§€ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ë‹¤ìŒ ë‘ ê°œì˜ ë‹¨ì–´ ì„¤ëª…ì„ **ê°ê´€ì  ê¸°ì¤€ì— ë”°ë¼ ë¹„êµ í‰ê°€**í•˜ê³ , ì ìˆ˜ë¥¼ ë§¤ê¸´ í›„ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.

        ---
        ### **ğŸ“Š í‰ê°€ ê¸°ì¤€ (ê°ê° 1~10ì ìœ¼ë¡œ í‰ê°€)**
        1ï¸. **ì •í™•ì„± (Accuracy)**: ì„¤ëª…ì´ ë‹¨ì–´ì˜ ì‚¬ì „ì  ì •ì˜ì™€ ì˜ ì¼ì¹˜í•˜ëŠ”ê°€?
        2ï¸. **ëª…í™•ì„± (Clarity)**: ì„¤ëª…ì´ ëª¨í˜¸í•˜ì§€ ì•Šê³  ì‰½ê²Œ ì´í•´ë˜ëŠ”ê°€? ì‰¬ìš´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í–ˆëŠ”ê°€? (Flesch-Kincaid ì§€ìˆ˜ ê³ ë ¤)
        3ï¸. **ì°½ì˜ì„± / ë‹¤ì–‘ì„± (Creativity & Diversity)**: ì—¬ëŸ¬ ì„¤ëª… ë°©ì‹(ë¹„ìœ , ìŠ¤í† ë¦¬í…”ë§ ë“±)ì„ í™œìš©í–ˆëŠ”ê°€?
        4ï¸. **ì ì ˆì„± (Appropriateness)**: ì˜ˆë¬¸ì´ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì™€ ë§ì•„ë–¨ì–´ì§€ëŠ”ê°€?

        ---
        ### **ì›ë³¸ HCX ëª¨ë¸ ì¶œë ¥ (íŠœë‹ ì „)**
        {original_data}

        ---
        ### **íŠœë‹ëœ HCX ëª¨ë¸ ì¶œë ¥ (íŠœë‹ í›„)**
        {tuned_data}

        ---
        ### **ì ìˆ˜ í‰ê°€ ë°©ì‹**
        1. **íŠœë‹ ì „**ê³¼ **íŠœë‹ í›„** ê°ê°ì˜ ì ìˆ˜ë¥¼ 1~10ì ìœ¼ë¡œ ë§¤ê¸°ì„¸ìš”.
        2. ìµœì¢…ì ìœ¼ë¡œ **íŠœë‹ í›„ ëª¨ë¸ì´ ì´ ëª‡ì  ê°œì„ ë˜ì—ˆëŠ”ì§€ ìš”ì•½**í•˜ì„¸ìš”.

        ---
        ### **ìµœì¢… ì¶œë ¥ í˜•ì‹  (ë°˜ë“œì‹œ JSON í˜•íƒœë¡œ ì¶œë ¥)**
        ì¶œë ¥ ê²°ê³¼ëŠ” ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë°˜ë“œì‹œ ì‘ì„±í•˜ì„¸ìš”. ê·¸ ì´ì™¸ì˜ í˜•ì‹ì€ ì ˆëŒ€ ì•ˆë©ë‹ˆë‹¤.

        ```json
        {{
        "evaluation": {{
            "accuracy": {{"before": X, "after": X}},
            "clarity": {{"before": X, "after": X}},
            "creativity": {{"before": X, "after": X}},
            "appropriateness": {{"before": X, "after": X}}
        }},
        "final_score": "íŠœë‹ í›„ ëª¨ë¸ì´ ì´ XXì  í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤."
        }}
        ```
        """

        response = self.query_gemini(prompt)

        # ì‘ë‹µì´ ë¬¸ìì—´ì´ë©´ JSON ë³€í™˜, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if isinstance(response, str):
            try:
                evaluation = json.loads(response)
                return evaluation
            except json.JSONDecodeError:
                print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨: ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        else:
            evaluation = response  # ì´ë¯¸ dictì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return evaluation

    def calculate_average_scores(self, evaluation_results):
        """ì „ì²´ í‰ê°€ ê²°ê³¼ì˜ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°"""
        total_scores = {
            "accuracy": {"before": 0, "after": 0},
            "clarity": {"before": 0, "after": 0},
            "creativity": {"before": 0, "after": 0},
            "appropriateness": {"before": 0, "after": 0},
        }

        num_samples = len(evaluation_results)

        # ëª¨ë“  ë‹¨ì–´ì˜ ì ìˆ˜ í•©ì‚°
        for result in evaluation_results:
            for criterion in total_scores.keys():
                total_scores[criterion]["before"] += result["evaluation"][criterion][
                    "before"
                ]
                total_scores[criterion]["after"] += result["evaluation"][criterion][
                    "after"
                ]

        # í‰ê·  ê³„ì‚°
        average_scores = {
            criterion: {
                "before": round(total_scores[criterion]["before"] / num_samples, 2),
                "after": round(total_scores[criterion]["after"] / num_samples, 2),
            }
            for criterion in total_scores.keys()
        }

        print("\nğŸ“Š **í‰ê·  í‰ê°€ ì ìˆ˜ (íŠœë‹ ì „ vs. íŠœë‹ í›„)**")
        print(json.dumps(average_scores, indent=4, ensure_ascii=False))

        return average_scores

    def evaluate_dataset(self, original_jsonl, tuned_jsonl, output_jsonl):
        """ë°ì´í„°ì…‹ ì „ì²´ë¥¼ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥"""
        with open(original_jsonl, "r", encoding="utf-8") as file1, open(
            tuned_jsonl, "r", encoding="utf-8"
        ) as file2:
            original_data = [json.loads(line) for line in file1]
            tuned_data = [json.loads(line) for line in file2]

        evaluation_results = []

        for orig, tuned in zip(original_data, tuned_data):
            time.sleep(2)
            # ë‹¨ì–´ë³„ í‰ê°€ ìˆ˜í–‰
            evaluation = self.evaluate_response(orig, tuned)

            # JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
            result = {
                "vocab": orig["vocab"],
                "evaluation": evaluation["evaluation"],
                "final_score": evaluation["final_score"],
            }

            evaluation_results.append(result)
            print(result)
            print(f"âœ… {orig['vocab']} í‰ê°€ ì™„ë£Œ")

        print(f"\nğŸ“Š ì „ì²´ í‰ê°€ ì™„ë£Œ!")

        # í‰ê°€ ê²°ê³¼ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥
        with open(output_jsonl, "w", encoding="utf-8") as jsonl_file:
            for result in evaluation_results:
                jsonl_file.write(json.dumps(result, ensure_ascii=False) + "\n")

        print(f"\nğŸ“‚ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_jsonl}")

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        self.calculate_average_scores(evaluation_results)

        return evaluation_results


if __name__ == "__main__":
    evaluator = VocabExplainEvaluator()
    evaluation_results = evaluator.evaluate_dataset(
        "./data/HCX_arabugi_vocab_explain.jsonl",
        "./data/2wk3wvmg_arabugi_vocab_explain.jsonl",
        "./data/arabugi_vocab_explain_evaluation.jsonl",
    )
